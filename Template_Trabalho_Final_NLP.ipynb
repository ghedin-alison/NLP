{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghedin-alison/NLP/blob/main/Template_Trabalho_Final_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "345377 - Alison Junior Ghedin<br>\n",
        "345946 - Leonardo Sarzedas do Carmo Vieira<br>\n",
        "344526 - Michael Douglas Barbosa Araujo<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "###**Criar um classificador de chamados aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "A **DinDinAgora** tem um canal de atendimento via chat e precisar classificar os assuntos dos atendimentos para melhorar as tratativas dos chamados dos clientes. O canal recebe textos abertos dos clientes relatando o problema e/ou dúvida e depois é direcionado para algum uma área especialista no assunto para uma melhor tratativa.​\n",
        "\n",
        "Crie um modelo classificador de assuntos aplicando técnicas de PLN, que consiga classificar através de um texto o assunto conforme disponível na base de dados [1] para treinamento e validação do modelo seu modelo.​\n",
        "\n",
        "O modelo precisar atingir um score na **métrica F1 Score superior a 75%**. Utilize o dataset [1] para treinar e testar o modelo, separe o dataset em duas amostras (75% para treinamento e 25% para teste com o randon_state igual a 42).​\n",
        "\n",
        "Fique à vontade para testar e explorar as técnicas de pré-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decisões durante o desenvolvimento.​\n",
        "\n",
        "**Composição da nota:​**\n",
        "\n",
        "**50%** - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, organização do pipeline, etc.)​\n",
        "\n",
        "**50%** - Baseado na performance (score) obtida com a amostra de teste no pipeline do modelo campeão (validar com  a Métrica F1 Score). **Separar o pipeline completo do modelo campeão conforme template.​**\n",
        "\n",
        "**[1] = ​https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv**\n",
        "\n",
        "Obs.: Para a métrica F1 Score, usar o parâmetro average = 'weighted'.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instalações de pacotes\n",
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtg6yHwG0vy",
        "outputId": "628c8ac0-541e-4aa4-de52-b271bc9159c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.5.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (67.6.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "2023-03-26 20:04:53.596562: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-26 20:04:56.284665: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-26 20:04:56.284812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-26 20:04:56.284840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'pt' are deprecated. Please use the\n",
            "full pipeline package name 'pt_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.5.0/pt_core_news_sm-3.5.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from pt-core-news-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (67.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import de bibliotecas\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "\n"
      ],
      "metadata": {
        "id": "Mla3DhPTF80U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "source": [
        "# CARREGANDO O DATA FRAME\n",
        "df = pd.read_csv('https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv', delimiter=';')\n",
        "\n",
        "# Façam o download do arquivo e utilizem localmente durante os testes"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s__lBzDQwrcG",
        "outputId": "ba34aa00-eee7-4107-bae8-fa0f6cd53353"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21072 entries, 0 to 21071\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   id_reclamacao         21072 non-null  int64 \n",
            " 1   data_abertura         21072 non-null  object\n",
            " 2   categoria             21072 non-null  object\n",
            " 3   descricao_reclamacao  21072 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 658.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "Bom desenvolvimento!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Verificando se há desbalanceamento das bases"
      ],
      "metadata": {
        "id": "ot5dUC9XE_ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.categoria.value_counts()"
      ],
      "metadata": {
        "id": "YeIMQT-2OSp-",
        "outputId": "2313b44a-3872-4ed0-e2ba-242180e4e3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Serviços de conta bancária             5161\n",
              "Cartão de crédito / Cartão pré-pago    5006\n",
              "Roubo / Relatório de disputa           4822\n",
              "Hipotecas / Empréstimos                3850\n",
              "Outros                                 2233\n",
              "Name: categoria, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanceando a base de dados"
      ],
      "metadata": {
        "id": "ZHCXV5msFxc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample da amostra\n",
        "sample_df = df.sample(frac=1,random_state=4)\n",
        "outros_df = sample_df.loc[sample_df.categoria==\"Outros\"]\n",
        "conta_df = sample_df.loc[sample_df.categoria==\"Serviços de conta bancária\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "roubo_df = sample_df.loc[sample_df.categoria==\"Roubo / Relatório de disputa\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "cartao_df = sample_df.loc[sample_df.categoria==\"Cartão de crédito / Cartão pré-pago\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "emprestimos_df = sample_df.loc[sample_df.categoria==\"Hipotecas / Empréstimos\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2LQ0cpR2Q-0B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.categoria.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuudWawJdUEA",
        "outputId": "88e0be71-cb19-4704-cca0-f75a2e7c63ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Serviços de conta bancária             5161\n",
              "Cartão de crédito / Cartão pré-pago    5006\n",
              "Roubo / Relatório de disputa           4822\n",
              "Hipotecas / Empréstimos                3850\n",
              "Outros                                 2233\n",
              "Name: categoria, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([outros_df, conta_df, roubo_df, cartao_df, emprestimos_df])\n",
        "df = df.sample(frac=1,random_state=42)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yKsFfvvBWFZ7",
        "outputId": "2bec9980-7e63-4bdd-80ce-4ffd4f18b3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id_reclamacao              data_abertura  \\\n",
              "12438        2477745  2017-05-07T12:00:00-05:00   \n",
              "2292         4188418  2021-03-06T12:00:00-05:00   \n",
              "10569        1502439  2015-08-04T12:00:00-05:00   \n",
              "20805        2834040  2018-03-05T12:00:00-05:00   \n",
              "18835        2125959  2016-09-22T12:00:00-05:00   \n",
              "\n",
              "                                 categoria  \\\n",
              "12438  Cartão de crédito / Cartão pré-pago   \n",
              "2292          Roubo / Relatório de disputa   \n",
              "10569                               Outros   \n",
              "20805                               Outros   \n",
              "18835           Serviços de conta bancária   \n",
              "\n",
              "                                    descricao_reclamacao  \n",
              "12438  Isso se refere à placa CHAS E/ XXXX XXXX XXXX ...  \n",
              "2292   Em xx/xx/2020, comprometi -me a comprar uma má...  \n",
              "10569  Violação da Lei da Verdade em Empréstimos/Regu...  \n",
              "20805  A edição do JP Morgan Chase envolve dinheiro e...  \n",
              "18835  Meu marido recebeu um visto de recompensa do A...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70c76940-988a-406c-9981-44a2f889a20a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_reclamacao</th>\n",
              "      <th>data_abertura</th>\n",
              "      <th>categoria</th>\n",
              "      <th>descricao_reclamacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12438</th>\n",
              "      <td>2477745</td>\n",
              "      <td>2017-05-07T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Isso se refere à placa CHAS E/ XXXX XXXX XXXX ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2292</th>\n",
              "      <td>4188418</td>\n",
              "      <td>2021-03-06T12:00:00-05:00</td>\n",
              "      <td>Roubo / Relatório de disputa</td>\n",
              "      <td>Em xx/xx/2020, comprometi -me a comprar uma má...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10569</th>\n",
              "      <td>1502439</td>\n",
              "      <td>2015-08-04T12:00:00-05:00</td>\n",
              "      <td>Outros</td>\n",
              "      <td>Violação da Lei da Verdade em Empréstimos/Regu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20805</th>\n",
              "      <td>2834040</td>\n",
              "      <td>2018-03-05T12:00:00-05:00</td>\n",
              "      <td>Outros</td>\n",
              "      <td>A edição do JP Morgan Chase envolve dinheiro e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18835</th>\n",
              "      <td>2125959</td>\n",
              "      <td>2016-09-22T12:00:00-05:00</td>\n",
              "      <td>Serviços de conta bancária</td>\n",
              "      <td>Meu marido recebeu um visto de recompensa do A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70c76940-988a-406c-9981-44a2f889a20a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70c76940-988a-406c-9981-44a2f889a20a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70c76940-988a-406c-9981-44a2f889a20a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "AX07aaL9XEd5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "9Gqbb4hvHTHX",
        "outputId": "e7112c7b-f45a-4768-ea10-f1e6a447ba71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id_reclamacao              data_abertura  \\\n",
              "0            2477745  2017-05-07T12:00:00-05:00   \n",
              "1            4188418  2021-03-06T12:00:00-05:00   \n",
              "2            1502439  2015-08-04T12:00:00-05:00   \n",
              "3            2834040  2018-03-05T12:00:00-05:00   \n",
              "4            2125959  2016-09-22T12:00:00-05:00   \n",
              "...              ...                        ...   \n",
              "11160        2142135  2016-10-03T12:00:00-05:00   \n",
              "11161        3341890  2019-08-07T12:00:00-05:00   \n",
              "11162        3585631  2020-03-30T12:00:00-05:00   \n",
              "11163        3939092  2020-11-05T12:00:00-05:00   \n",
              "11164        2972458  2018-07-24T12:00:00-05:00   \n",
              "\n",
              "                                 categoria  \\\n",
              "0      Cartão de crédito / Cartão pré-pago   \n",
              "1             Roubo / Relatório de disputa   \n",
              "2                                   Outros   \n",
              "3                                   Outros   \n",
              "4               Serviços de conta bancária   \n",
              "...                                    ...   \n",
              "11160         Roubo / Relatório de disputa   \n",
              "11161         Roubo / Relatório de disputa   \n",
              "11162         Roubo / Relatório de disputa   \n",
              "11163                               Outros   \n",
              "11164  Cartão de crédito / Cartão pré-pago   \n",
              "\n",
              "                                    descricao_reclamacao  \n",
              "0      Isso se refere à placa CHAS E/ XXXX XXXX XXXX ...  \n",
              "1      Em xx/xx/2020, comprometi -me a comprar uma má...  \n",
              "2      Violação da Lei da Verdade em Empréstimos/Regu...  \n",
              "3      A edição do JP Morgan Chase envolve dinheiro e...  \n",
              "4      Meu marido recebeu um visto de recompensa do A...  \n",
              "...                                                  ...  \n",
              "11160  Um cartão de crédito emitido pelo Chase Bank d...  \n",
              "11161  Entrei em contato com o Chase Bank sobre meu i...  \n",
              "11162  Transferi um saldo {$ 5000.00} para o meu cart...  \n",
              "11163  CHASE AUTO CONTA NÚMERO XXXX XXXX XXXX As decl...  \n",
              "11164  Em XX/XX/2017, recebi uma carta do Chase Bank ...  \n",
              "\n",
              "[11165 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ad6f76e9-151b-4c9d-b240-9ec70f506587\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_reclamacao</th>\n",
              "      <th>data_abertura</th>\n",
              "      <th>categoria</th>\n",
              "      <th>descricao_reclamacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2477745</td>\n",
              "      <td>2017-05-07T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Isso se refere à placa CHAS E/ XXXX XXXX XXXX ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4188418</td>\n",
              "      <td>2021-03-06T12:00:00-05:00</td>\n",
              "      <td>Roubo / Relatório de disputa</td>\n",
              "      <td>Em xx/xx/2020, comprometi -me a comprar uma má...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1502439</td>\n",
              "      <td>2015-08-04T12:00:00-05:00</td>\n",
              "      <td>Outros</td>\n",
              "      <td>Violação da Lei da Verdade em Empréstimos/Regu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2834040</td>\n",
              "      <td>2018-03-05T12:00:00-05:00</td>\n",
              "      <td>Outros</td>\n",
              "      <td>A edição do JP Morgan Chase envolve dinheiro e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2125959</td>\n",
              "      <td>2016-09-22T12:00:00-05:00</td>\n",
              "      <td>Serviços de conta bancária</td>\n",
              "      <td>Meu marido recebeu um visto de recompensa do A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11160</th>\n",
              "      <td>2142135</td>\n",
              "      <td>2016-10-03T12:00:00-05:00</td>\n",
              "      <td>Roubo / Relatório de disputa</td>\n",
              "      <td>Um cartão de crédito emitido pelo Chase Bank d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11161</th>\n",
              "      <td>3341890</td>\n",
              "      <td>2019-08-07T12:00:00-05:00</td>\n",
              "      <td>Roubo / Relatório de disputa</td>\n",
              "      <td>Entrei em contato com o Chase Bank sobre meu i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11162</th>\n",
              "      <td>3585631</td>\n",
              "      <td>2020-03-30T12:00:00-05:00</td>\n",
              "      <td>Roubo / Relatório de disputa</td>\n",
              "      <td>Transferi um saldo {$ 5000.00} para o meu cart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11163</th>\n",
              "      <td>3939092</td>\n",
              "      <td>2020-11-05T12:00:00-05:00</td>\n",
              "      <td>Outros</td>\n",
              "      <td>CHASE AUTO CONTA NÚMERO XXXX XXXX XXXX As decl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11164</th>\n",
              "      <td>2972458</td>\n",
              "      <td>2018-07-24T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Em XX/XX/2017, recebi uma carta do Chase Bank ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11165 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad6f76e9-151b-4c9d-b240-9ec70f506587')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad6f76e9-151b-4c9d-b240-9ec70f506587 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad6f76e9-151b-4c9d-b240-9ec70f506587');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uID-EB4wGJb3",
        "outputId": "bf3247ba-3d2c-426b-e28a-c1a689cd0248"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11165, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.categoria.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEYLtyJNIO26",
        "outputId": "cf4a0f04-eb53-4a71-8b2f-036f7a17479f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cartão de crédito / Cartão pré-pago    2233\n",
              "Roubo / Relatório de disputa           2233\n",
              "Outros                                 2233\n",
              "Serviços de conta bancária             2233\n",
              "Hipotecas / Empréstimos                2233\n",
              "Name: categoria, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verificar o texto e o que poderia ser filtrado"
      ],
      "metadata": {
        "id": "kEjdGUcpF8J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.descricao_reclamacao[1]"
      ],
      "metadata": {
        "id": "WowYZlu2H4ou",
        "outputId": "9369e48d-0406-486d-d9ba-a395013747e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Em xx/xx/2020, comprometi -me a comprar uma máquina de corte XXXX XXXX usada em xxxx (item # xxxx,) e o processo de checkout xxxx utilizado para enviar o pagamento com meu cartão de crédito Chase. O valor da transação, incluindo frete, era {$ 1000,00}. O vendedor enviou um pacote contendo um tijolo, lavanderia suja e lixo e forneceu um número de rastreamento xxxx. Quando o pacote chegou ao nosso endereço comercial, sua entrega foi confirmada pela assinatura. Depois que o driver XXXX deixou, abri a embalagem e descobri que ele não continha a máquina de corte xxxx xxxx e senti que houve uma tentativa deliberada do vendedor de me fraudar.\\r\\n\\r\\nEm xx/xx/2020, enviei uma disputa via xxxx e selecionei a opção que não recebi o item. Na minha opinião, essa foi absolutamente uma declaração verdadeira, pois não recebi o item que me comprometi a comprar. Infelizmente, isso fez com que o sistema de resolução XXXX faça referência ao número de rastreamento xxxx fornecido pelo scammer e regra a seu favor. É verdade que recebi um pacote via XXXX, mas o pacote era um instrumento de fraude, recheado com lixo e não continha o item que eu havia comprometido a comprar.\\r\\nTentei repetidamente contestar a decisão no site XXXX, mas o link fornecia me levou em círculos e, finalmente, não me permitiu enviar uma disputa. Tentei explicar a situação a um representante xxxx via telefone, mas eles só me instruíram a seguir os links quebrados no site.\\r\\n\\r\\nEm xx/xx/2020, arquivei um relatório no sistema de reclamação de fraude de correio de serviços postais dos EUA (número de caso XXXX.) Até agora, a janela de devolução XXXX de 30 dias havia fechado e qualquer tentativa adicional de devolver o item exigiria a aprovação da aprovação do vendedor. O vendedor, é claro, é um golpista e não estaria disposto a aprovar um retorno. Tentei novamente ligar para o XXXX Customer Service, mas eles haviam fechado seus call centers devido à pandemia CoviD-19. Tentei me comunicar com o XXXX Atendimento ao cliente por e -mail, mas recebi apenas respostas automatizadas.\\r\\n\\r\\nNesse ponto, senti que havia chegado a um beco sem saída com xxxx e decidi registrar uma disputa com o Chase. No decorrer de sua investigação, o pessoal da Chase alcançou o XXXX e o XXXX forneceu as informações de rastreamento XXXX. Chase decidiu que a acusação era válida e eu não recebi um reembolso.\\r\\n\\r\\nFinalmente consegui alcançar um representante de atendimento ao cliente no XXXX que revisou cuidadosamente as evidências que eu o forneci e reconheci a fraude, mas ele me disse que não conseguiu prosseguir com um reembolso, pois já havia iniciado uma disputa com perseguição. XXXX encerrou a conta do vendedor, que eu vejo como um reconhecimento de que ele é culpado de violar seus termos de serviço.\\r\\n\\r\\nExaminei o próprio pacote e descobri que ele não continha fisicamente o item descrito na listagem XXXX, principalmente se fosse acolchoado com materiais de embalagem de qualquer tipo. Além disso, o peso do pacote (impresso na etiqueta XXXX em uma fotografia fornecida pelo Scammer via sistema de mensagens XXXX XXXX como prova de ter enviado o item) é menor que o peso geral da própria máquina, excluindo qualquer materiais de embalagem. Além disso, o local do XXXX Office em XXXX, WA (no recibo na foto) está a mais de 85 milhas do endereço de retorno XXXX, WA, o scammer usado na etiqueta XXXX. Minha pesquisa não rendeu nenhum residente pelo nome fornecido (xxxx xxxx) no endereço de retorno fornecido. Tentei ligar para o número de telefone fornecido no endereço de retorno e não recebi resposta. Suspeito que o endereço de retorno e o número de telefone foram escolhidos aleatoriamente pelo scammer.\\r\\n\\r\\nO pacote e seu conteúdo ainda estão em minha posse, pois não houve resolução satisfatória dessa provação. Como mencionado anteriormente, o Serviço de Inspeção Postal dos EUA foi notificado. Acredito que o valor da transação colocaria isso na categoria de fraude criminosa, se os USPIS estivessem inclinados a encontrar o golpista e o processo. Acolhei o pessoal da Chase para enviar um representante para vir à minha residência e inspecionar o pacote em questão, mas eles não fizeram nenhuma tentativa de fazê -lo.\\r\\n\\r\\nO departamento de fraude interna da Chase decidiu repetidamente que a acusação é legítima e, toda vez, reenviei a disputa. A disputa mais recente permaneceu sem resposta por meses, e acho que eles esperam que eu simplesmente desapareça e pare de seguir um reembolso por essa acusação fraudulenta. Enquanto isso, parei de usar seus serviços completamente e permiti que minhas contas fiquem inativas enquanto utilizo os serviços de outros provedores de cartão de crédito.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load da biblioteca spacy"
      ],
      "metadata": {
        "id": "AkqBk4rDJg9h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "source": [
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de limpeza de texto removendo:\n",
        "pontuação, quebra de linha, informações sensíveis, valores, acentuação"
      ],
      "metadata": {
        "id": "_wGC4zwYJoCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remover o XXXX\n",
        "def limpar_textos(texto):\n",
        "  texto = re.sub(r'[,.:;]', '', texto)\n",
        "  texto = re.sub(r'\\n', ' ', texto)\n",
        "  texto = re.sub(r'[xxxx]', '', texto)\n",
        "  texto = re.sub(r'[XXXX]', '', texto)\n",
        "  texto = re.sub(r'[xx/xx/xxxx]', '', texto)\n",
        "  texto = re.sub(r'\\{.*?\\}', '', texto)\n",
        "  texto = re.sub('\\\\s+', ' ', texto)\n",
        "  texto = re.sub(u'[^a-zA-ZéúíóáÉÚÍÓÁèùìòàÈÙÌÒÀõãñÕÃÑêûîôâÊÛÎÔÂëÿüïöäËYÜÏÖÄçÇ ]', '', texto)\n",
        "  texto = texto.strip().lower()\n",
        "  return texto"
      ],
      "metadata": {
        "id": "G39iZMQjZuQc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções de lematização de texto completo e de verbos"
      ],
      "metadata": {
        "id": "N5unfF53J_-j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue0nV0uVo3OZ"
      },
      "source": [
        "# função de lematização completa do documento\n",
        "def lematiza_texto(texto: str):\n",
        "  sent = []\n",
        "  doc = nlp(texto)\n",
        "  for word in doc:\n",
        "    sent.append(word.lemma_)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# função de lematização de verbos do documento\n",
        "def lematiza_verbos(texto:str):\n",
        "  sent = []\n",
        "  doc = nlp(texto)\n",
        "  for word in doc:\n",
        "    if word.pos_ == \"VERB\":\n",
        "      sent.append(word.lemma_)\n",
        "    else:\n",
        "      sent.append(word.text)\n",
        "  return \" \".join(sent)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_treat\"] = df.descricao_reclamacao.apply(limpar_textos)"
      ],
      "metadata": {
        "id": "F1oBcHJGb60b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text_treat.head()"
      ],
      "metadata": {
        "id": "27eXlrY-j2tx",
        "outputId": "41fb91c8-48e9-4872-e90c-91c49c8dccaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    isso se refere à placa chas e eu me inscrevi n...\n",
              "1    em  comprometi me a comprar uma máquina de cor...\n",
              "2    violação da lei da verdade em empréstimosregul...\n",
              "3    a edição do jp morgan chase envolve dinheiro e...\n",
              "4    meu marido recebeu um visto de recompensa do a...\n",
              "Name: text_treat, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criar uma amostra dos dados para uma execução de testes mais rápida."
      ],
      "metadata": {
        "id": "6kuQb4mcYEM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw, df_amostra = train_test_split(df, \n",
        "                                       test_size=0.1, \n",
        "                                       random_state=42)"
      ],
      "metadata": {
        "id": "Fb_KJhhKynrM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatização de texto"
      ],
      "metadata": {
        "id": "5occOqO9YOx8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "source": [
        "df_amostra[\"text_lemma\"] = df_amostra.text_treat.apply(lematiza_texto)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatização de verbos"
      ],
      "metadata": {
        "id": "ObW3iKWFYRl8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25cBRwGAw8-1"
      },
      "source": [
        "df_amostra[\"text_lemma_verbos\"] = df_amostra.text_treat.apply(lematiza_verbos)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separação de amostra entre treino e teste"
      ],
      "metadata": {
        "id": "IlvXa0QrYlWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# divisão das amostras em treino e teste\n",
        "df_treino, df_teste = train_test_split(df_amostra, \n",
        "                                       test_size=0.3, \n",
        "                                       random_state=42)"
      ],
      "metadata": {
        "id": "ONAbwPqj9Dpi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 1\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - TFid<br> \n",
        "  - Frequencia dos termos invertida<br>\n",
        "  - Ngram: Unigrama<br> \n",
        "  - Norm: l2<br> \n",
        "Lematização:\n",
        "  - Texto  \n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "bc84jBxNYtam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "stops_spacy = nlp.Defaults.stop_words\n",
        "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
        "\n",
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops, norm='l2')\n",
        "vetor_tfid.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma)\n",
        "\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma)\n",
        "pred = linear_model.predict(text_vect_test)\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Sf8Bv69b8g",
        "outputId": "c3e2e287-0fc1-48b8-d986-474314abf975"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8756101045316731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 2\n",
        "StopWords: Spacy<br> \n",
        "Vetorização: \n",
        "  - TFid<br> \n",
        "  - Ngram: combinação Unigrama/Bigrama<br> \n",
        "  - Norm: l1\n",
        "Lematização:\n",
        "  - Verbos  \n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "_0BraCSOaWWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,2), use_idf=False, stop_words=list(stops_spacy), norm='l1')\n",
        "vetor_tfid.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma)\n",
        "\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma)\n",
        "pred = linear_model.predict(text_vect_test)\n",
        "\n",
        "accuracy = accuracy_score(pred, df_teste['categoria'])\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCAW-3nb2Y2v",
        "outputId": "b34e7c9d-0869-4333-e7fa-a72ccbdc60b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8071385375630659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 3\n",
        "StopWords: NLTK<br> \n",
        "Vetorização: \n",
        "  - TFid<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "  - Norm: l2<br> \n",
        "Lematização:\n",
        "  - Texto  \n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "5FA4iRdoeP8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,1), use_idf=False, stop_words=stops_nltk, norm='l2')\n",
        "vetor_tfid.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma)\n",
        "\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma)\n",
        "pred = linear_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_iOXVVj3iCn",
        "outputId": "6737ae2f-6440-4443-f3e2-c8b1cf424d2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8606966029219033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 4\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "Lematização:\n",
        "  - Texto  \n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "MgS8c5gjepPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma)\n",
        "pred = linear_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLo3XwyS308B",
        "outputId": "f3c202c8-9325-4b09-8481-5487e008d93a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.856248209989192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 5\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "Lematização:\n",
        "  - Verbos<br>\n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "69A5rFfDfdpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma_verbos)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbos)\n",
        "\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma_verbos)\n",
        "pred = linear_model.predict(text_vect_test)\n",
        "\n",
        "accuracy = accuracy_score(pred, df_teste['categoria'])\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpud93th4h04",
        "outputId": "72371632-d937-4bc0-e5e6-000252a2e9f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8537029660223244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 6\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "Lematização:\n",
        "  - Verbos<br>\n",
        "Modelo:\n",
        "  - Decision Tree"
      ],
      "metadata": {
        "id": "kR_ZTpGIgVLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma_verbos)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbos)\n",
        "\n",
        "tree_model = DecisionTreeClassifier()\n",
        "tree_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma_verbos)\n",
        "pred = tree_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txIwCcfg58mH",
        "outputId": "329283b0-4694-4767-87c7-4b20dbc762b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.676605332728094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 7\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Combinação Unigrama/Bigrama<br> \n",
        "Lematização:\n",
        "  - Verbos<br>\n",
        "Modelo:\n",
        "  - Decision Tree"
      ],
      "metadata": {
        "id": "rdMfscrMgiqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma_verbos)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma_verbos)\n",
        "\n",
        "tree_model = DecisionTreeClassifier()\n",
        "tree_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma_verbos)\n",
        "pred = tree_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaQXDlol6YQ_",
        "outputId": "6058b874-38a0-4de5-e480-770c6e47b642"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.6739597613878562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 8\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "Lematização:\n",
        "  - Texto<br>\n",
        "Modelo:\n",
        "  - Decision Tree"
      ],
      "metadata": {
        "id": "TjkDXOV1g-XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "tree_model = DecisionTreeClassifier()\n",
        "tree_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma)\n",
        "pred = tree_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iwuMzUB6kBC",
        "outputId": "c482218d-924a-439c-969d-3f6aefe3d806"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.6935512530859246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 9\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Unigrama<br> \n",
        "Lematização:\n",
        "  - Texto<br>\n",
        "Modelo:\n",
        "  - Random Forest"
      ],
      "metadata": {
        "id": "vtiXkdMwhc6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "trees_model = RandomForestClassifier()\n",
        "trees_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma)\n",
        "pred = trees_model.predict(text_vect_test)\n",
        "\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwr_uCzp7Hqn",
        "outputId": "5e6c9b57-8e4a-46e1-df18-8dee54dbdfdf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.7908913442278104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 10\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - CountVectorizer<br> \n",
        "  - Ngram: Combinação Unigrama/Bigrama<br> \n",
        "Lematização:\n",
        "  - Texto<br>\n",
        "Modelo:\n",
        "  - Random Forest"
      ],
      "metadata": {
        "id": "whT24qQKiblY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)\n",
        "vect.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vect.transform(df_treino.text_lemma)\n",
        "\n",
        "trees_model = RandomForestClassifier()\n",
        "trees_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vect.transform(df_teste.text_lemma)\n",
        "pred = trees_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUQEh2Qh7d4-",
        "outputId": "4fcbb47f-daec-49dc-9408-e908ed1dd766"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8013795180266223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 11\n",
        "StopWords: NLTK e Spacy<br> \n",
        "Vetorização: \n",
        "  - TFid<br> \n",
        "  - Frequencia dos termos invertida<br>\n",
        "  - Ngram: Unigrama<br> \n",
        "  - Norm: l2<br> \n",
        "Lematização:\n",
        "  - Verbos  \n",
        "Modelo:\n",
        "  - Random Forest"
      ],
      "metadata": {
        "id": "OBooFjFDi3iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops, norm='l2')\n",
        "\n",
        "vetor_tfid.fit(df_treino.text_lemma_verbos)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma_verbos)\n",
        "\n",
        "trees_model = RandomForestClassifier()\n",
        "trees_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma_verbos)\n",
        "pred = trees_model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQLkB3N674nD",
        "outputId": "7d9c9866-8c1b-4c60-c778-ad69080bc37a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8034786060519918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste 12\n",
        "StopWords: NLTK<br> \n",
        "Vetorização: \n",
        "  - TFid<br> \n",
        "  - Frequencia dos termos invertida<br>\n",
        "  - Ngram: Combinação Unigrama/Bigrama<br> \n",
        "  - Norm: l2<br> \n",
        "Lematização:\n",
        "  - Texto  \n",
        "Modelo:\n",
        "  - Regressão Logistica"
      ],
      "metadata": {
        "id": "6WYwlVjdjc0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=list(stops_nltk), norm='l2')\n",
        "vetor_tfid.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma)\n",
        "\n",
        "\n",
        "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma)\n",
        "pred = model.predict(text_vect_test)\n",
        "\n",
        "print(f\"F1 Score: {f1_score(df_teste['categoria'], pred, average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuEei6l6_TRU",
        "outputId": "de9a86f1-3094-4ec8-ff30-4c0ae7c4a5ca"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8990351159424108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "####**Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação de pacotes"
      ],
      "metadata": {
        "id": "xWXPqGJX5bKc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxqHA-XCrqsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c4ff86-4663-42a6-dd76-02fe4f3f39de"
      },
      "source": [
        "# instalações de pacotes\n",
        "!pip install spacy\n",
        "!python -m spacy download pt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (67.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/spacy/errors.py\", line 2, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/spacy/compat.py\", line 3, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/thinc/config.py\", line 4, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/thinc/types.py\", line 8, in <module>\n",
            "    from .compat import has_cupy, cupy\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/thinc/compat.py\", line 30, in <module>\n",
            "    import torch.utils.dlpack\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/__init__.py\", line 831, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/functional.py\", line 8, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/__init__.py\", line 2, in <module>\n",
            "    from .linear import Identity, Linear, Bilinear, LazyLinear\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 7, in <module>\n",
            "    from .. import functional as F\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 18, in <module>\n",
            "    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/_jit_internal.py\", line 40, in <module>\n",
            "    import torch.package._mangling as package_mangling\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/package/__init__.py\", line 1, in <module>\n",
            "    from .analyze.is_from_package import is_from_package\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/package/analyze/__init__.py\", line 1, in <module>\n",
            "    from .find_first_use_of_broken_modules import find_first_use_of_broken_modules\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/package/analyze/find_first_use_of_broken_modules.py\", line 3, in <module>\n",
            "    from ..package_exporter import PackagingError\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/package/package_exporter.py\", line 32, in <module>\n",
            "    from ._importlib import _normalize_path\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 935, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1079, in path_stats\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 142, in _path_stat\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import de bibliotecas"
      ],
      "metadata": {
        "id": "sNWFZ3WB5qyl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFA-CYfawkEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "354fc7ef-ca7b-4b90-8725-f4bb309b3f6c"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import nltk\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-165d8d94013a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# set library-specific custom warning handling before doing anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msetup_default_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/errors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mErrorsWithCodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/thinc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/thinc/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPromise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVARIABLE_RE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/thinc/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/thinc/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdlpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mout_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_broadcast_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;31m# TODO: add docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m copysign = _make_elementwise_binary_reference(\n\u001b[0m\u001b[1;32m    977\u001b[0m     \u001b[0m_copysign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0mtype_promotion_kind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINT_TO_FLOAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m_make_elementwise_binary_reference\u001b[0;34m(prim, type_promotion_kind, aten_op, has_out, supports_lhs_python_scalar, supports_rhs_python_scalar, disable_meta)\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0maten_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maten_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0mregister_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mdecomposition_decorator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_op_to_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0mType2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0mType2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36madd_op_to_table\u001b[0;34m(aten_op)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# TODO: factor this logic into OpOverload or Library API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_overload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mop_overload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverload_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0mname\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mop_overload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverload_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregar a base de dados"
      ],
      "metadata": {
        "id": "44g9y__c5xO0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJtvcfXo3J4"
      },
      "source": [
        "df = pd.read_csv('https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv', delimiter=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resample da base de dados"
      ],
      "metadata": {
        "id": "55Cq2lmVFjiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df = df.sample(frac=1,random_state=4)\n",
        "outros_df = sample_df.loc[sample_df.categoria==\"Outros\"]\n",
        "conta_df = sample_df.loc[sample_df.categoria==\"Serviços de conta bancária\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "roubo_df = sample_df.loc[sample_df.categoria==\"Roubo / Relatório de disputa\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "cartao_df = sample_df.loc[sample_df.categoria==\"Cartão de crédito / Cartão pré-pago\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "emprestimos_df = sample_df.loc[sample_df.categoria==\"Hipotecas / Empréstimos\"].sample(n=outros_df.shape[0], random_state=42)\n",
        "df = pd.concat([outros_df, conta_df, roubo_df, cartao_df, emprestimos_df])\n",
        "df = df.sample(frac=1,random_state=42)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4avrE7o1FN4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregar a biblioteca spacy"
      ],
      "metadata": {
        "id": "k7WbdtNl55zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pt_core_news_sm')\n"
      ],
      "metadata": {
        "id": "gnPSaqmr5nS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de limpeza de texto"
      ],
      "metadata": {
        "id": "fFz_Ifp36BRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpar_textos(texto):\n",
        "  texto = re.sub(r'[,.:;]', '', texto)\n",
        "  texto = re.sub(r'\\n', ' ', texto)\n",
        "  texto = re.sub(r'[xxxx]', '', texto)\n",
        "  texto = re.sub(r'[XXXX]', '', texto)\n",
        "  texto = re.sub(r'\\{.*?\\}', '', texto)\n",
        "  texto = re.sub(r'[xx/xx/xxxx]', '', texto)\n",
        "  texto = re.sub('\\\\s+', ' ', texto)\n",
        "  texto = re.sub(u'[^a-zA-ZéúíóáÉÚÍÓÁèùìòàÈÙÌÒÀõãñÕÃÑêûîôâÊÛÎÔÂëÿüïöäËYÜÏÖÄçÇ ]', '', texto)\n",
        "  texto = texto.strip().lower()\n",
        "  return texto"
      ],
      "metadata": {
        "id": "-KEm30bnndz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de Lematização de texto"
      ],
      "metadata": {
        "id": "pPaWrNlv6JNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematiza_texto(texto: str):\n",
        "  sent = []\n",
        "  doc = nlp(texto)\n",
        "  for word in doc:\n",
        "    sent.append(word.lemma_)\n",
        "  return \" \".join(sent)"
      ],
      "metadata": {
        "id": "qNhztPvqnO9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aplicando limpeza e lematização do texto"
      ],
      "metadata": {
        "id": "ZMtGpLTV6Wck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text_lemma\"] = df.descricao_reclamacao.apply(limpar_textos).apply(lematiza_texto)"
      ],
      "metadata": {
        "id": "w0hcoiHYnj8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separação de dados entre treino e teste"
      ],
      "metadata": {
        "id": "29gUS1Ge6fAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_treino, df_teste = train_test_split(df, \n",
        "                                       test_size=0.25, \n",
        "                                       random_state=42)"
      ],
      "metadata": {
        "id": "otGNEF_ZpVzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execução do modelo campeão\n",
        "Utilizamos stopwords do NLTK, vetor combinando unigrama e bigrama, e modelo de Regressão Logística"
      ],
      "metadata": {
        "id": "FujlYVeR6p8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "#stopwords combinadas\n",
        "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização com Tfid\n",
        "\n",
        "vetor_tfid = TfidfVectorizer(ngram_range=(1,2), use_idf=True, stop_words=list(stops_nltk), norm='l2')\n",
        "vetor_tfid.fit(df_treino.text_lemma)\n",
        "text_vect_treino = vetor_tfid.transform(df_treino.text_lemma)\n",
        "\n",
        "#treina modelo\n",
        "linear_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "linear_model.fit(text_vect_treino, df_treino[\"categoria\"])\n",
        "\n",
        "# testa e verifica a predição\n",
        "text_vect_test = vetor_tfid.transform(df_teste.text_lemma)\n",
        "pred = linear_model.predict(text_vect_test)"
      ],
      "metadata": {
        "id": "Eg8vQqqPpDFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medida de avaliação\n",
        "Utilizamos o F1 Score como medida de avaliação do modelo"
      ],
      "metadata": {
        "id": "WO7jUzPf7FBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(df_teste['categoria'], pred, average='weighted')"
      ],
      "metadata": {
        "id": "ThGMwjP_7Dpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}